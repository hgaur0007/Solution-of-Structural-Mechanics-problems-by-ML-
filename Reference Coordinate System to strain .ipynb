{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb48b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb2024a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RCS_original= np.array([[-1, -0.987063428, -0.974129442, -0.961198042, -0.948269226, -0.935342994, -0.922419344, -0.909498275, -0.896579787, -0.870750547, -0.735315981, -0.664487066, -0.600164371, -0.51664037, -0.394760493, -0.330705304, -0.260317778, -0.196395511,-0.126153894, -0.062363997, 0.007732313, 0.071390387, 0.141341991, 0.204868786, 0.27467628, 0.338072335, 0.407736312, 0.471002165, 0.540523214, 0.603659398, 0.673038104, 0.73604515, 0.805282094, 0.817862672, 0.937256291, 1.000006639]])\n",
    "t_strain_original=np.array([[0, 0.019998, 0.039992, 0.059982, 0.079968, 0.09995, 0.1199281, 0.1399021, 0.1598721, 0.1998003, 0.4091618, 0.5186527, 0.6180859, 0.7472015, 0.9356095, 1.0346292, 1.1434378, 1.242252, 1.350835, 1.4494446, 1.557803, 1.6562088, 1.7643435, 1.8625464, 1.9704583, 2.0684591, 2.1761492, 2.2739487, 2.3814178, 2.4790169, 2.586266,2.6836654, 2.7906953, 2.810143, 2.9947076, 3.0917103]])\n",
    "t_stress_original= np.array([[0, 0.340068, 0.710284, 1.10066, 1.5012, 1.96196, 2.332796, 2.65371, 3.114976, 3.3066, 3.805539, 4.0208, 4.22604, 4.373075, 4.532206, 4.607424, 4.683245, 4.7385, 4.794328, 4.849788, 4.885517, 4.910661, 4.916864, 4.921744, 4.926117, 4.930947, 4.93626, 4.94109, 4.946403, 4.951233, 4.956546, 4.960832, 4.965274, 4.9666, 4.983792, 4.996952]])\n",
    "\n",
    "RCS = RCS_original.T\n",
    "t_strain=t_strain_original.T\n",
    "t_stress = t_stress_original.T\n",
    "\n",
    "#print(t_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1bee86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, \n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "        \n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 *self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "      \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0  \n",
    "      \n",
    "class Activation_Linear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()       \n",
    "        \n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.02, decay=0.0001, epsilon=1e-7, beta_1=0.9, beta_2=0.99):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights        \n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases  \n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))       \n",
    "        bias_momentums_corrected = layer.bias_momentums /(1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2        \n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache /(1 - self.beta_2 ** (self.iterations + 1))        \n",
    "        bias_cache_corrected = layer.bias_cache /(1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate *weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)   \n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected /(np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "        \n",
    "class Loss:\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dca3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.0, loss: 229.72676990785556 (data_loss: 229.72676990785556, reg_loss: 0), lr: 0.02\n",
      "epoch: 1000, acc: 0.1388888888888889, loss: 0.00031813444871485957 (data_loss: 0.00031813444871485957, reg_loss: 0), lr: 0.018183471224656786\n",
      "epoch: 2000, acc: 0.08333333333333333, loss: 0.0019100567972241422 (data_loss: 0.0019100567972241422, reg_loss: 0), lr: 0.016668055671305942\n",
      "epoch: 3000, acc: 0.1111111111111111, loss: 0.0006532309293014073 (data_loss: 0.0006532309293014073, reg_loss: 0), lr: 0.015385798907608278\n",
      "epoch: 4000, acc: 0.08333333333333333, loss: 0.0030548802909534573 (data_loss: 0.0030548802909534573, reg_loss: 0), lr: 0.014286734766769053\n",
      "epoch: 5000, acc: 0.0, loss: 0.015074831391981276 (data_loss: 0.015074831391981276, reg_loss: 0), lr: 0.013334222281485434\n",
      "epoch: 6000, acc: 0.16666666666666666, loss: 0.0003818240598658123 (data_loss: 0.0003818240598658123, reg_loss: 0), lr: 0.01250078129883118\n",
      "epoch: 7000, acc: 0.027777777777777776, loss: 0.0016183240544022932 (data_loss: 0.0016183240544022932, reg_loss: 0), lr: 0.011765397964586153\n",
      "epoch: 8000, acc: 0.05555555555555555, loss: 0.004976558650328856 (data_loss: 0.004976558650328856, reg_loss: 0), lr: 0.011111728429357186\n",
      "epoch: 9000, acc: 0.7222222222222222, loss: 1.1081014777697247e-05 (data_loss: 1.1081014777697247e-05, reg_loss: 0), lr: 0.010526869835254487\n",
      "epoch: 10000, acc: 0.1111111111111111, loss: 0.0008761506037531223 (data_loss: 0.0008761506037531223, reg_loss: 0), lr: 0.010000500025001252\n",
      "epoch: 11000, acc: 0.2777777777777778, loss: 0.00011681168211901507 (data_loss: 0.00011681168211901507, reg_loss: 0), lr: 0.009524263060145723\n",
      "epoch: 12000, acc: 0.16666666666666666, loss: 0.0005638103522168664 (data_loss: 0.0005638103522168664, reg_loss: 0), lr: 0.00909132233283331\n",
      "epoch: 13000, acc: 0.6944444444444444, loss: 8.990666657590351e-06 (data_loss: 8.990666657590351e-06, reg_loss: 0), lr: 0.008696030262185312\n",
      "epoch: 14000, acc: 0.2777777777777778, loss: 7.095990999793811e-05 (data_loss: 7.095990999793811e-05, reg_loss: 0), lr: 0.00833368057002375\n",
      "epoch: 15000, acc: 0.25, loss: 0.0001779312797453211 (data_loss: 0.0001779312797453211, reg_loss: 0), lr: 0.008000320012800512\n",
      "epoch: 16000, acc: 0.19444444444444445, loss: 0.0002196523595966651 (data_loss: 0.0002196523595966651, reg_loss: 0), lr: 0.007692603561675449\n",
      "epoch: 17000, acc: 1.0, loss: 1.8193750682721651e-06 (data_loss: 1.8193750682721651e-06, reg_loss: 0), lr: 0.007407681765991332\n",
      "epoch: 18000, acc: 0.4444444444444444, loss: 2.0084883275616744e-05 (data_loss: 2.0084883275616744e-05, reg_loss: 0), lr: 0.007143112254009072\n",
      "epoch: 19000, acc: 0.9722222222222222, loss: 5.7478765545729195e-06 (data_loss: 5.7478765545729195e-06, reg_loss: 0), lr: 0.00689678954446705\n",
      "epoch: 20000, acc: 0.7222222222222222, loss: 1.687948376232554e-05 (data_loss: 1.687948376232554e-05, reg_loss: 0), lr: 0.006666888896296542\n",
      "epoch: 21000, acc: 1.0, loss: 2.543371475362784e-07 (data_loss: 2.543371475362784e-07, reg_loss: 0), lr: 0.006451821026484724\n",
      "epoch: 22000, acc: 0.9722222222222222, loss: 1.3022022090772217e-06 (data_loss: 1.3022022090772217e-06, reg_loss: 0), lr: 0.006250195318603706\n",
      "epoch: 23000, acc: 0.7777777777777778, loss: 1.3261178847562223e-05 (data_loss: 1.3261178847562223e-05, reg_loss: 0), lr: 0.006060789720900634\n",
      "epoch: 24000, acc: 0.3611111111111111, loss: 3.770314244175908e-05 (data_loss: 3.770314244175908e-05, reg_loss: 0), lr: 0.005882525956645783\n",
      "epoch: 25000, acc: 0.1111111111111111, loss: 0.0004386160178009959 (data_loss: 0.0004386160178009959, reg_loss: 0), lr: 0.005714448984256693\n",
      "epoch: 26000, acc: 0.4722222222222222, loss: 0.00029856129886011583 (data_loss: 0.00029856129886011583, reg_loss: 0), lr: 0.0055557098808300225\n",
      "epoch: 27000, acc: 0.9166666666666666, loss: 5.608171169236467e-06 (data_loss: 5.608171169236467e-06, reg_loss: 0), lr: 0.0054055515013919295\n",
      "epoch: 28000, acc: 1.0, loss: 1.3139328257654268e-06 (data_loss: 1.3139328257654268e-06, reg_loss: 0), lr: 0.005263296402536909\n",
      "epoch: 29000, acc: 0.4722222222222222, loss: 2.5878881053352493e-05 (data_loss: 2.5878881053352493e-05, reg_loss: 0), lr: 0.005128336624016\n",
      "epoch: 30000, acc: 1.0, loss: 2.2953489663226493e-07 (data_loss: 2.2953489663226493e-07, reg_loss: 0), lr: 0.005000125003125078\n",
      "epoch: 31000, acc: 1.0, loss: 4.835786958566278e-06 (data_loss: 4.835786958566278e-06, reg_loss: 0), lr: 0.004878167760189274\n",
      "epoch: 32000, acc: 0.75, loss: 1.4265400491621657e-05 (data_loss: 1.4265400491621657e-05, reg_loss: 0), lr: 0.004762018143289126\n",
      "epoch: 33000, acc: 1.0, loss: 4.309376277939787e-07 (data_loss: 4.309376277939787e-07, reg_loss: 0), lr: 0.004651270959789762\n",
      "epoch: 34000, acc: 0.7777777777777778, loss: 1.0130158864863904e-05 (data_loss: 1.0130158864863904e-05, reg_loss: 0), lr: 0.00454555785358758\n",
      "epoch: 35000, acc: 0.2777777777777778, loss: 6.982341496685694e-05 (data_loss: 6.982341496685694e-05, reg_loss: 0), lr: 0.004444543212071379\n",
      "epoch: 36000, acc: 0.75, loss: 1.4695400806972275e-05 (data_loss: 1.4695400806972275e-05, reg_loss: 0), lr: 0.004347920606969717\n",
      "epoch: 37000, acc: 1.0, loss: 3.2778524987711806e-06 (data_loss: 3.2778524987711806e-06, reg_loss: 0), lr: 0.004255409689567864\n",
      "epoch: 38000, acc: 1.0, loss: 7.865129666814391e-07 (data_loss: 7.865129666814391e-07, reg_loss: 0), lr: 0.004166753474030709\n",
      "epoch: 39000, acc: 1.0, loss: 7.464369681262467e-07 (data_loss: 7.464369681262467e-07, reg_loss: 0), lr: 0.004081715953386804\n",
      "epoch: 40000, acc: 0.8888888888888888, loss: 6.515609711585513e-06 (data_loss: 6.515609711585513e-06, reg_loss: 0), lr: 0.004000080001600032\n",
      "epoch: 41000, acc: 1.0, loss: 2.3387509206276415e-07 (data_loss: 2.3387509206276415e-07, reg_loss: 0), lr: 0.003921645522461225\n",
      "epoch: 42000, acc: 0.9166666666666666, loss: 5.495315306886231e-06 (data_loss: 5.495315306886231e-06, reg_loss: 0), lr: 0.003846227812073309\n",
      "epoch: 43000, acc: 1.0, loss: 2.306747941656141e-06 (data_loss: 2.306747941656141e-06, reg_loss: 0), lr: 0.0037736561067189946\n",
      "epoch: 44000, acc: 0.8888888888888888, loss: 5.869130731747677e-06 (data_loss: 5.869130731747677e-06, reg_loss: 0), lr: 0.0037037722920794827\n",
      "epoch: 45000, acc: 1.0, loss: 6.93107798024838e-08 (data_loss: 6.93107798024838e-08, reg_loss: 0), lr: 0.0036364297532682412\n",
      "epoch: 46000, acc: 1.0, loss: 6.028024322424428e-07 (data_loss: 6.028024322424428e-07, reg_loss: 0), lr: 0.0035714923480776447\n",
      "epoch: 47000, acc: 0.9444444444444444, loss: 4.369389400461985e-06 (data_loss: 4.369389400461985e-06, reg_loss: 0), lr: 0.0035088334883068124\n",
      "epoch: 48000, acc: 0.8888888888888888, loss: 6.034892003893682e-06 (data_loss: 6.034892003893682e-06, reg_loss: 0), lr: 0.00344833531612614\n",
      "epoch: 49000, acc: 1.0, loss: 5.562266656641695e-07 (data_loss: 5.562266656641695e-07, reg_loss: 0), lr: 0.003389887964202783\n",
      "epoch: 50000, acc: 1.0, loss: 1.8248055278171066e-08 (data_loss: 1.8248055278171066e-08, reg_loss: 0), lr: 0.00333338888981483\n",
      "epoch: 51000, acc: 1.0, loss: 2.1841795119726027e-06 (data_loss: 2.1841795119726027e-06, reg_loss: 0), lr: 0.003278742274463516\n",
      "epoch: 52000, acc: 1.0, loss: 1.0128305537308053e-07 (data_loss: 1.0128305537308053e-07, reg_loss: 0), lr: 0.0032258584815884122\n",
      "epoch: 53000, acc: 0.7222222222222222, loss: 2.519238075012748e-05 (data_loss: 2.519238075012748e-05, reg_loss: 0), lr: 0.0031746535659296178\n",
      "epoch: 54000, acc: 0.75, loss: 1.886072213017412e-05 (data_loss: 1.886072213017412e-05, reg_loss: 0), lr: 0.0031250488288879514\n",
      "epoch: 55000, acc: 1.0, loss: 8.093844736410892e-08 (data_loss: 8.093844736410892e-08, reg_loss: 0), lr: 0.00307697041492946\n",
      "epoch: 56000, acc: 0.7222222222222222, loss: 2.0883810650141733e-05 (data_loss: 2.0883810650141733e-05, reg_loss: 0), lr: 0.00303034894468098\n",
      "epoch: 57000, acc: 1.0, loss: 4.4218853944407874e-07 (data_loss: 4.4218853944407874e-07, reg_loss: 0), lr: 0.0029851191808832967\n",
      "epoch: 58000, acc: 0.8333333333333334, loss: 9.191875633094537e-06 (data_loss: 9.191875633094537e-06, reg_loss: 0), lr: 0.002941219723819468\n",
      "epoch: 59000, acc: 0.9722222222222222, loss: 3.7798467355000074e-06 (data_loss: 3.7798467355000074e-06, reg_loss: 0), lr: 0.0028985927332280175\n",
      "epoch: 60000, acc: 0.9444444444444444, loss: 4.074986827847153e-06 (data_loss: 4.074986827847153e-06, reg_loss: 0), lr: 0.0028571836740524863\n",
      "epoch: 61000, acc: 1.0, loss: 5.725375923812162e-07 (data_loss: 5.725375923812162e-07, reg_loss: 0), lr: 0.002816941083677235\n",
      "epoch: 62000, acc: 0.75, loss: 1.4016713438757247e-05 (data_loss: 1.4016713438757247e-05, reg_loss: 0), lr: 0.0027778163585605352\n",
      "epoch: 63000, acc: 0.9722222222222222, loss: 3.9958806413773e-06 (data_loss: 3.9958806413773e-06, reg_loss: 0), lr: 0.0027397635584049096\n",
      "epoch: 64000, acc: 1.0, loss: 6.080274616503628e-07 (data_loss: 6.080274616503628e-07, reg_loss: 0), lr: 0.0027027392262057594\n",
      "epoch: 65000, acc: 1.0, loss: 5.514366375535108e-08 (data_loss: 5.514366375535108e-08, reg_loss: 0), lr: 0.0026667022226963028\n",
      "epoch: 66000, acc: 1.0, loss: 1.1303363156193172e-06 (data_loss: 1.1303363156193172e-06, reg_loss: 0), lr: 0.0026316135738628137\n",
      "epoch: 67000, acc: 1.0, loss: 1.434526243549201e-07 (data_loss: 1.434526243549201e-07, reg_loss: 0), lr: 0.0025974363303419524\n",
      "epoch: 68000, acc: 1.0, loss: 2.3765267667929896e-06 (data_loss: 2.3765267667929896e-06, reg_loss: 0), lr: 0.0025641354376338163\n",
      "epoch: 69000, acc: 0.7777777777777778, loss: 1.311806413385201e-05 (data_loss: 1.311806413385201e-05, reg_loss: 0), lr: 0.0025316776161723566\n",
      "epoch: 70000, acc: 0.5, loss: 2.7409007705686056e-05 (data_loss: 2.7409007705686056e-05, reg_loss: 0), lr: 0.00250003125039063\n",
      "epoch: 71000, acc: 1.0, loss: 6.052676244334898e-07 (data_loss: 6.052676244334898e-07, reg_loss: 0), lr: 0.0024691662860035304\n",
      "epoch: 72000, acc: 1.0, loss: 7.344174035889394e-07 (data_loss: 7.344174035889394e-07, reg_loss: 0), lr: 0.0024390541348065222\n",
      "epoch: 73000, acc: 1.0, loss: 3.68438919813253e-08 (data_loss: 3.68438919813253e-08, reg_loss: 0), lr: 0.002409667586356462\n",
      "epoch: 74000, acc: 1.0, loss: 2.164517141189337e-07 (data_loss: 2.164517141189337e-07, reg_loss: 0), lr: 0.0023809807259610235\n",
      "epoch: 75000, acc: 0.7777777777777778, loss: 1.320503259184649e-05 (data_loss: 1.320503259184649e-05, reg_loss: 0), lr: 0.002352968858457158\n",
      "epoch: 76000, acc: 1.0, loss: 2.61347929241723e-09 (data_loss: 2.61347929241723e-09, reg_loss: 0), lr: 0.00232560843730741\n",
      "epoch: 77000, acc: 1.0, loss: 1.551707925918597e-07 (data_loss: 1.551707925918597e-07, reg_loss: 0), lr: 0.0022988769985861908\n",
      "epoch: 78000, acc: 1.0, loss: 8.035612247787336e-08 (data_loss: 8.035612247787336e-08, reg_loss: 0), lr: 0.0022727530994670394\n",
      "epoch: 79000, acc: 1.0, loss: 3.890467685999105e-08 (data_loss: 3.890467685999105e-08, reg_loss: 0), lr: 0.0022472162608568637\n",
      "epoch: 80000, acc: 1.0, loss: 1.7673894642450474e-06 (data_loss: 1.7673894642450474e-06, reg_loss: 0), lr: 0.0022222469138545985\n",
      "epoch: 81000, acc: 1.0, loss: 4.079759233738635e-07 (data_loss: 4.079759233738635e-07, reg_loss: 0), lr: 0.002197826349740107\n",
      "epoch: 82000, acc: 0.9722222222222222, loss: 4.178210717887594e-06 (data_loss: 4.178210717887594e-06, reg_loss: 0), lr: 0.002173936673224709\n",
      "epoch: 83000, acc: 1.0, loss: 3.0069510596232415e-07 (data_loss: 3.0069510596232415e-07, reg_loss: 0), lr: 0.0021505607587178357\n",
      "epoch: 84000, acc: 1.0, loss: 3.035629577351572e-07 (data_loss: 3.035629577351572e-07, reg_loss: 0), lr: 0.002127682209385206\n",
      "epoch: 85000, acc: 1.0, loss: 1.4956156473494143e-07 (data_loss: 1.4956156473494143e-07, reg_loss: 0), lr: 0.0021052853187928295\n",
      "epoch: 86000, acc: 1.0, loss: 1.1858796331451016e-07 (data_loss: 1.1858796331451016e-07, reg_loss: 0), lr: 0.0020833550349482807\n",
      "epoch: 87000, acc: 1.0, loss: 1.5315978409854635e-08 (data_loss: 1.5315978409854635e-08, reg_loss: 0), lr: 0.002061876926566253\n",
      "epoch: 88000, acc: 1.0, loss: 2.877571280382028e-06 (data_loss: 2.877571280382028e-06, reg_loss: 0), lr: 0.002040837151399504\n",
      "epoch: 89000, acc: 0.8888888888888888, loss: 5.950367496659526e-06 (data_loss: 5.950367496659526e-06, reg_loss: 0), lr: 0.0020202224264891564\n",
      "epoch: 90000, acc: 1.0, loss: 7.80094528905054e-07 (data_loss: 7.80094528905054e-07, reg_loss: 0), lr: 0.0020000200002000023\n",
      "epoch: 91000, acc: 1.0, loss: 9.701916501729762e-07 (data_loss: 9.701916501729762e-07, reg_loss: 0), lr: 0.0019802176259170884\n",
      "epoch: 92000, acc: 1.0, loss: 6.494730452941777e-07 (data_loss: 6.494730452941777e-07, reg_loss: 0), lr: 0.001960803537289581\n",
      "epoch: 93000, acc: 1.0, loss: 7.688245998470704e-07 (data_loss: 7.688245998470704e-07, reg_loss: 0), lr: 0.0019417664249167467\n",
      "epoch: 94000, acc: 1.0, loss: 1.2755042845631058e-06 (data_loss: 1.2755042845631058e-06, reg_loss: 0), lr: 0.0019230954143789844\n",
      "epoch: 95000, acc: 1.0, loss: 1.024492577205063e-06 (data_loss: 1.024492577205063e-06, reg_loss: 0), lr: 0.001904780045524243\n",
      "epoch: 96000, acc: 1.0, loss: 3.8511937040418214e-07 (data_loss: 3.8511937040418214e-07, reg_loss: 0), lr: 0.0018868102529269143\n",
      "epoch: 97000, acc: 1.0, loss: 2.212482206657696e-06 (data_loss: 2.212482206657696e-06, reg_loss: 0), lr: 0.0018691763474424992\n",
      "epoch: 98000, acc: 1.0, loss: 9.384655006662877e-08 (data_loss: 9.384655006662877e-08, reg_loss: 0), lr: 0.0018518689987870257\n",
      "epoch: 99000, acc: 1.0, loss: 1.1075423065831975e-07 (data_loss: 1.1075423065831975e-07, reg_loss: 0), lr: 0.0018348792190754043\n"
     ]
    }
   ],
   "source": [
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Linear()\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_Adam()\n",
    "\n",
    "accuracy_precision = np.std(t_strain) / 250\n",
    "\n",
    "for epoch in range(100000):\n",
    "    dense1.forward(RCS)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "    data_loss = loss_function.calculate(activation3.output, t_strain)\n",
    "    \n",
    "    regularization_loss =loss_function.regularization_loss(dense1) +loss_function.regularization_loss(dense2) +loss_function.regularization_loss(dense3)\n",
    "    \n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - t_strain) <accuracy_precision)\n",
    "    \n",
    "    if not epoch % 1000:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy}, ' +\n",
    "              f'loss: {loss} (' +\n",
    "              f'data_loss: {data_loss}, ' +\n",
    "              f'reg_loss: {regularization_loss}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, t_strain)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd31f25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_test = np.array([[-1, -0.987063428, -0.974129442, -0.961198042, -0.948269226, -0.935342994, -0.922419344, -0.909498275, -0.896579787, -0.870750547, -0.735315981, -0.664487066, -0.600164371, -0.51664037, -0.394760493, -0.330705304, -0.260317778, -0.196395511,-0.126153894, -0.062363997, 0.007732313, 0.071390387, 0.141341991, 0.204868786, 0.27467628, 0.338072335, 0.407736312, 0.471002165, 0.540523214, 0.603659398, 0.673038104, 0.73604515, 0.805282094, 0.817862672, 0.937256291, 1.000006639]])\n",
    "X_test= X_test.T\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "\n",
    "plt.plot(RCS , t_strain)\n",
    "plt.scatter(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21461ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02407946]]\n"
     ]
    }
   ],
   "source": [
    "dense1.forward(-0.9852653)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "print(activation3.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7ec6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac12e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f0a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
